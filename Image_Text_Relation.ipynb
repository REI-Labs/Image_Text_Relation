{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d892cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-22 12:13:42.317451: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "from PIL import Image\n",
    "import spacy\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fd28300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-] Skipping image 1 on page 0 due to its small size.\n",
      "[-] Skipping image 2 on page 0 due to its small size.\n",
      "[-] Skipping image 1 on page 7 due to its small size.\n",
      "[-] Skipping image 2 on page 7 due to its small size.\n",
      "[-] Skipping image 3 on page 7 due to its small size.\n",
      "[-] Skipping image 4 on page 7 due to its small size.\n",
      "[-] Skipping image 1 on page 8 due to its small size.\n",
      "[-] Skipping image 2 on page 8 due to its small size.\n",
      "[-] Skipping image 1 on page 52 due to its small size.\n",
      "[-] Skipping image 2 on page 52 due to its small size.\n",
      "[-] Skipping image 3 on page 52 due to its small size.\n",
      "[-] Skipping image 4 on page 52 due to its small size.\n",
      "[-] Skipping image 1 on page 53 due to its small size.\n",
      "[-] Skipping image 2 on page 53 due to its small size.\n"
     ]
    }
   ],
   "source": [
    "# file path you want to extract images from\n",
    "file = \"267980655.pdf\"\n",
    "# open the file\n",
    "pdf_file = fitz.open(file)\n",
    "\n",
    "# Output directory for the extracted images\n",
    "output_dir = \"extracted_images\"\n",
    "# Desired output image format\n",
    "output_format = \"png\"\n",
    "# Minimum width and height for extracted images\n",
    "min_width = 1000\n",
    "min_height = 1000\n",
    "# Create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# Iterate over PDF pages\n",
    "for page_index in range(len(pdf_file)):\n",
    "    # Get the page itself\n",
    "    page = pdf_file[page_index]\n",
    "    # Get image list\n",
    "    image_list = page.get_images(full=True)\n",
    "    # Print the number of images found on this page\n",
    "    # if image_list:\n",
    "        # print(f\"[+] Found a total of {len(image_list)} images in page {page_index}\")\n",
    "    # else:\n",
    "        # print(f\"[!] No images found on page {page_index}\")\n",
    "    # Iterate over the images on the page\n",
    "    for image_index, img in enumerate(image_list, start=1):\n",
    "        # Get the XREF of the image\n",
    "        xref = img[0]\n",
    "        # Extract the image bytes\n",
    "        base_image = pdf_file.extract_image(xref)\n",
    "        image_bytes = base_image[\"image\"]\n",
    "        # Get the image extension\n",
    "        image_ext = base_image[\"ext\"]\n",
    "        # Load it to PIL\n",
    "        image = Image.open(io.BytesIO(image_bytes))\n",
    "        # Check if the image meets the minimum dimensions and save it\n",
    "        if image.width >= min_width and image.height >= min_height:\n",
    "            image.save(\n",
    "                open(os.path.join(output_dir, f\"image{page_index + 1}_{image_index}.{output_format}\"), \"wb\"),\n",
    "                format=output_format.upper())\n",
    "        else:\n",
    "            print(f\"[-] Skipping image {image_index} on page {page_index} due to its small size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200433ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "raw_image = Image.open(\"extracted_images/image8_3.png\").convert('RGB')\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"trademark of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "text = processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "out = model.generate(**inputs)\n",
    "text2 = processor.decode(out[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc32fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the starbucks logo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.replace('trademark of','')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0166b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n",
    "    \n",
    "nouns = [token.lemma_ for token in doc if token.pos_ == \"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec35f404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'starbuck'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7799e3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN dep xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "477170b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "doc = fitz.open('267980655.pdf')\n",
    "text = \"\"\n",
    "for page in doc:\n",
    "    text+=page.get_text()\n",
    "text = ' '.join(text.split()).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e95d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = []\n",
    "\n",
    "for t in text.split('. '):\n",
    "    if nouns[0] in t:\n",
    "        relevant.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c44ccbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['research in conceptual metaphor theory finds, though, that metaphor is “primarily a matter of thought and action and only derivatively a matter of language.” indeed, brands rely not just on verbal metaphor, but also on visual metaphor to differentiate themselves from competitors in the marketplace (e.g., target’s “bullseye” and starbucks’s “siren”)',\n",
       " '2018] visual metaphor and distinctiveness 773 marketplace.15 examples include apple’s logo,16 starbucks’s siren,17 and nike’s swoosh.18 as with verbal metaphor in the word mark context, use of visual metaphor enables an image mark to serve as an inherent source identifier by (1) denoting (referring literally to) a brand, as well as (2) 15',\n",
       " 'starbucks corp., starbucks logo, 2011; audio2visual contest, a siren’s call: a little bit closer, contest image, 2014, https://www.flickr.com/photos/rinoa_cathcart [https://perma.cc/bv7q-8zlt]',\n",
       " 'consider the common examples mentioned in the introduction: starbucks’s siren logo, nike’s swoosh, and the apple logo',\n",
       " '93:767 as illustrations, let us once more return to the examples presented in the introduction: (1) the apple logo,272 (2) starbucks’s siren,273 and (3) 272',\n",
       " 'starbucks corp., starbucks logo, 2011; audio2visual contest, a siren’s call: a little bit closer, contest image, 2014, https://www.flickr.com/photos/rinoa_cathcart [https://perma.cc/bv7q-8zlt]',\n",
       " 'the apple logo is quite clearly a representation of an apple, and the starbucks siren represents a mermaid, and each would meet this factor of the test',\n",
       " 'the swoosh is a rough sketch of the goddess nike’s wing, and the starbucks logo is a visual representation of a siren/mermaid',\n",
       " 'the apple logo and starbucks’s siren would each satisfy this final factor',\n",
       " 'likewise, starbucks’s logo—a mermaid, or siren, with long, spiraling locks of hair—could be said to invoke the archetype of the explorer and 281',\n",
       " '93:767 the theme of the sea.282 in myth, sirens have long been associated with luring sailors in with enchanting songs, and starbucks similarly lures consumers in with the promise of a hot beverage and a recess from the daily grind.283 starbucks’s trade dress is likewise metaphorical']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd4bf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'PROPN': starbucks}, {'PROPN': .}, {'PROPN': logo}, {'NOUN': contest, 'ADJ': audio2visual}, {'NOUN': call, 'DET': a, 'ADJ': siren, 'PART': ’s}, {'NOUN': bit, 'DET': a, 'ADJ': little, 'ADV': closer, 'PUNCT': ,, 'VERB': contest}]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "txt = relevant[2]\n",
    "doc = nlp(txt)\n",
    "\n",
    "chunks = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    out = {}\n",
    "    root = chunk.root\n",
    "    out[root.pos_] = root\n",
    "    for tok in chunk:\n",
    "        if tok != root:\n",
    "            out[tok.pos_] = tok\n",
    "    chunks.append(out)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f806beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'NOUN': examples, 'DET': the, 'ADJ': common}, {'NOUN': introduction, 'DET': the}, {'NOUN': starbucks, 'PART': ’s, 'ADJ': siren}, {'PROPN': nike}, {'NOUN': apple, 'DET': the}]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "txt = relevant[3]\n",
    "doc = nlp(txt)\n",
    "\n",
    "chunks = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    out = {}\n",
    "    root = chunk.root\n",
    "    out[root.pos_] = root\n",
    "    for tok in chunk:\n",
    "        if tok != root:\n",
    "            out[tok.pos_] = tok\n",
    "    chunks.append(out)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca2a6f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
